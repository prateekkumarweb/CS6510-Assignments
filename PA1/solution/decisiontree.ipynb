{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Class Decision tree\n",
    "class DecisionTree :\n",
    "    class Node :\n",
    "        def __init__(self) :\n",
    "            self.feature = 0\n",
    "            self.class_no = -1\n",
    "            self.children = []\n",
    "    \n",
    "    def __init__(self) :\n",
    "        import numpy\n",
    "        self.np = numpy\n",
    "    \n",
    "    # Train on the dataset\n",
    "    def train(self, file) :\n",
    "        # Take dataset from file\n",
    "        dataset = self.np.genfromtxt(file, delimiter=',', dtype=\"|S50\", autostrip=True)\n",
    "        self.np.random.shuffle(dataset)\n",
    "        # Seperate class 0 and class 1 dataset and perform stratified splitting into train and validation\n",
    "        dataset_0 = dataset[dataset[:,14] == '0']\n",
    "        dataset_1 = dataset[dataset[:,14] == '1']\n",
    "        trainset = self.np.concatenate((dataset_0[:4*len(dataset_0)//5], dataset_1[:4*len(dataset_1)//5]), axis=0)\n",
    "        validationset = self.np.concatenate((dataset_0[4*len(dataset_0)//5:], dataset_1[4*len(dataset_1)//5:]), axis=0)\n",
    "        # Shuffle train and validation\n",
    "        self.np.random.shuffle(trainset)\n",
    "        self.np.random.shuffle(validationset)\n",
    "        # Preprocess train set\n",
    "        train = self.sanitize(trainset)\n",
    "        # Array to stroe number of children in each features that are possible\n",
    "        self.no_children = self.np.empty(14, dtype=self.np.int)\n",
    "        for i in xrange(14) :\n",
    "            self.no_children[i] = self.np.amax(train[:,i])\n",
    "        # Preprocess Valiodation set\n",
    "        valid = self.sanitize_valid(validationset)\n",
    "        valid[:,14] = validationset[:,14].astype(self.np.int)\n",
    "        self.train = train\n",
    "        self.valid_x = valid[:,:-1]\n",
    "        self.valid_y = valid[:,-1]\n",
    "        # Array to keep note of features that have been decied\n",
    "        decided_features = self.np.zeros(14)\n",
    "        # Learn recursively the tree\n",
    "        node = self.learn_node(self.train, decided_features)\n",
    "        self.node = node\n",
    "        \n",
    "    # Validate and predict the accuracy\n",
    "    def validate(self) :\n",
    "        correct_answers = 0\n",
    "        for i in xrange(len(self.valid_x)) :\n",
    "            if self.predict_row(self.valid_x[i]) == self.valid_y[i] :\n",
    "                correct_answers += 1\n",
    "        print \"Accuracy is\", float(correct_answers)/float(len(self.valid_x))\n",
    "    \n",
    "    # Predict on the given test_file and return the outputs\n",
    "    def predict(self, test_file) :\n",
    "        dataset = self.np.genfromtxt(test_file, delimiter=',', dtype=\"|S50\", autostrip=True, invalid_raise=False)\n",
    "        test_x = self.sanitize_valid(dataset)\n",
    "        test_y = self.np.empty(len(test_x), dtype=self.np.int)\n",
    "        for i in xrange(len(test_x)) :\n",
    "            test_y[i] = self.predict_row(test_x[i,:])\n",
    "        return test_y\n",
    "\n",
    "    # Predict one test case\n",
    "    def predict_row(self, test_row) :\n",
    "        # Recursively go through the nodes until you find the answer\n",
    "        node = self.node\n",
    "        while node.class_no == -1 :\n",
    "            fv = test_row[node.feature]\n",
    "            if fv < len(node.children) :\n",
    "                node = node.children[fv]\n",
    "            else :\n",
    "                node = node.children[-1]\n",
    "        return node.class_no        \n",
    "    \n",
    "    # Determine the entropy on the given feature from given dataset\n",
    "    def entropy(self, dataset, feature) :\n",
    "        n = float(len(dataset))\n",
    "        ent = 0.0\n",
    "        for i in xrange(self.no_children[feature]+1) :\n",
    "            ni = float(len(dataset[dataset[:,feature] == 1]))\n",
    "            pi = ni/n\n",
    "            if ni != 0.0 :\n",
    "                ent += -pi*self.np.log2(pi)\n",
    "        return ent\n",
    "    \n",
    "    # Learn nodes of decision tree recursively\n",
    "    def learn_node(self, dataset, decided_features) :\n",
    "        node = DecisionTree.Node()\n",
    "        # If no dataset left predict randomly\n",
    "        if len(dataset) == 0 :\n",
    "            node.class_no = self.np.random.randint(0, 1)\n",
    "            return node\n",
    "        # If all datasets belong to class 1 predict 1\n",
    "        if (len(dataset) == len(dataset[dataset[:,14] == 1])) :\n",
    "            node.class_no = 1\n",
    "            return node\n",
    "        # If all datasets belong to class 1 predict 2\n",
    "        elif (len(dataset) == len(dataset[dataset[:,14] == 0])) :\n",
    "            node.class_no = 0\n",
    "            return node\n",
    "        # Calculate feature whose entropy is least\n",
    "        minentropy = self.np.inf;\n",
    "        minentropy_feature = -1\n",
    "        for i in  xrange(14) :\n",
    "            if decided_features[i] == 0 :\n",
    "                ent = self.entropy(dataset, i)\n",
    "                if ent < minentropy :\n",
    "                    minentropy = ent\n",
    "                    minentropy_feature = i\n",
    "        # NO feature is foud then predict randomly and end\n",
    "        if minentropy_feature == -1 :\n",
    "            node.class_no = self.np.random.randint(0,1)\n",
    "            return node\n",
    "        # Split on feature with least entropy\n",
    "        node.feature = minentropy_feature\n",
    "        for i in xrange(self.no_children[i]+1) :\n",
    "            new_decided_features = self.np.empty(14)\n",
    "            new_decided_features[:] = decided_features\n",
    "            new_decided_features[node.feature] = 1\n",
    "            node.children.append(self.learn_node(dataset[dataset[:,node.feature] == i], new_decided_features))\n",
    "        return node\n",
    "     \n",
    "    # Sanitize train data\n",
    "    def sanitize(self, dataset) :\n",
    "        self.replacer = []\n",
    "        for i in xrange(len(dataset[0])) :\n",
    "            u, indices = self.np.unique(dataset[:, i], return_inverse=True)\n",
    "            self.replacer.append(u[self.np.argmax(self.np.bincount(indices))])\n",
    "        for i in xrange(len(dataset[0])) :\n",
    "            dataset[:,i][dataset[:,i] == '?'] = self.replacer[i]\n",
    "        data = self.np.empty((len(dataset), len(dataset[0])), dtype=self.np.int)\n",
    "        data[:,0] = dataset[:,0].astype(self.np.int)\n",
    "        for i in xrange(len(data)) :\n",
    "            data[i,0] = data[i,0] // 10\n",
    "        self.col1dict, data[:,1] = self.np.unique(dataset[:, 1], return_inverse=True)\n",
    "        data[:,2] = dataset[:,2].astype(self.np.int)\n",
    "        for i in xrange(len(data)) :\n",
    "            data[i,2] = data[i,2] // 25000\n",
    "        self.col3dict, data[:,3] = self.np.unique(dataset[:, 3], return_inverse=True)\n",
    "        data[:,4] = dataset[:,4].astype(self.np.int)\n",
    "        self.col5dict, data[:,5] = self.np.unique(dataset[:, 5], return_inverse=True)\n",
    "        self.col6dict, data[:,6] = self.np.unique(dataset[:, 6], return_inverse=True)\n",
    "        self.col7dict, data[:,7] = self.np.unique(dataset[:, 7], return_inverse=True)\n",
    "        self.col8dict, data[:,8] = self.np.unique(dataset[:, 8], return_inverse=True)\n",
    "        self.col9dict, data[:,9] = self.np.unique(dataset[:, 9], return_inverse=True)\n",
    "        data[:,10] = dataset[:,10].astype(self.np.int)\n",
    "        for i in xrange(len(data)) :\n",
    "            data[i,10] = data[i,10] // 5000\n",
    "        data[:,11] = dataset[:,11].astype(self.np.int)\n",
    "        for i in xrange(len(data)) :\n",
    "            data[i,11] = data[i,11] // 100\n",
    "        data[:,12] = dataset[:,12].astype(self.np.int)\n",
    "        for i in xrange(len(data)) :\n",
    "            data[i,12] = data[i,12] // 10\n",
    "        self.col13dict, data[:,13] = self.np.unique(dataset[:, 13], return_inverse=True)\n",
    "        data[:,14] = dataset[:,14].astype(self.np.int)\n",
    "        return data\n",
    "    \n",
    "    # Sanitize test and validation data\n",
    "    def sanitize_valid(self, dataset) :\n",
    "        data = self.np.empty((len(dataset), len(dataset[0])), dtype=self.np.int)\n",
    "        for i in xrange(len(dataset[0])) :\n",
    "            dataset[dataset[:, i] == '?', i] = self.replacer[i]\n",
    "        data[:,0] = dataset[:,0].astype(self.np.int)\n",
    "        for i in xrange(len(data)) :\n",
    "            data[i,0] = data[i,0] // 10\n",
    "        for i in xrange(len(self.col1dict)) :\n",
    "            data[:,1][dataset[:,1] == self.col1dict[i]] = i\n",
    "        data[:,2] = dataset[:,2].astype(self.np.int)\n",
    "        for i in xrange(len(data)) :\n",
    "            data[i,2] = data[i,2] // 25000\n",
    "        for i in xrange(len(self.col3dict)) :\n",
    "            data[:,3][dataset[:,3] == self.col3dict[i]] = i\n",
    "        data[:,4] = dataset[:,4].astype(self.np.int)\n",
    "        for i in xrange(len(self.col5dict)) :\n",
    "            data[:,5][dataset[:,5] == self.col5dict[i]] = i\n",
    "        for i in xrange(len(self.col6dict)) :\n",
    "            data[:,6][dataset[:,6] == self.col6dict[i]] = i\n",
    "        for i in xrange(len(self.col7dict)) :\n",
    "            data[:,7][dataset[:,7] == self.col7dict[i]] = i\n",
    "        for i in xrange(len(self.col8dict)) :\n",
    "            data[:,8][dataset[:,8] == self.col8dict[i]] = i\n",
    "        for i in xrange(len(self.col9dict)) :\n",
    "            data[:,9][dataset[:,9] == self.col9dict[i]] = i\n",
    "        data[:,10] = dataset[:,10].astype(self.np.int)\n",
    "        for i in xrange(len(data)) :\n",
    "            data[i,10] = data[i,10] // 5000\n",
    "        data[:,11] = dataset[:,11].astype(self.np.int)\n",
    "        for i in xrange(len(data)) :\n",
    "            data[i,11] = data[i,11] // 100\n",
    "        data[:,12] = dataset[:,12].astype(self.np.int)\n",
    "        for i in xrange(len(data)) :\n",
    "            data[i,12] = data[i,12] // 10\n",
    "        for i in xrange(len(self.col13dict)) :\n",
    "            data[:,13][dataset[:,13] == self.col13dict[i]] = i\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.792603698151\n"
     ]
    }
   ],
   "source": [
    "# Intialize decision tree, validate, pickle and save\n",
    "dt = DecisionTree()\n",
    "dt.train('data/train.csv')\n",
    "dt.validate()\n",
    "\n",
    "import dill\n",
    "with open('cs15btech11031.model', 'wb') as file:\n",
    "    dill.dump(dt, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 0.792603698151\n"
     ]
    }
   ],
   "source": [
    "with open('cs15btech11031.model', 'rb') as file:\n",
    "    st = dill.load(file)\n",
    "    st.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
